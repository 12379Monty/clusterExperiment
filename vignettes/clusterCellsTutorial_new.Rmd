---
title: "clusterExperiment Tutorial"
author: "Calvin Chi and Elizabeth Purdom"
date: "`r Sys.Date()`"
output: 
  BiocStyle::html_document
    toc: true
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

---
```{r GlobalOptions, results="hide", include=FALSE}
#rmarkdown::render("clusterExperimentTutorial.Rmd")
knitr::opts_chunk$set(fig.align="center", cache=TRUE, cache.path = "R_cache/", fig.path="R_figure/", fig.width=6,fig.height=6,error=TRUE,autodep=TRUE,out.width="600px",out.height="600px")
knitr::dep_auto()
library(diagram)
library(clusterExperiment)
```

```{r drawClusterDefinition}
diagSize<-"600px"
drawClusterAll <- function(highlight=NULL, Layer2Options=NULL,plotOptions=TRUE,Layer2OnlyIfUsed=TRUE){
	plotLayer2<-length(Layer2Options)>0 | !Layer2OnlyIfUsed
	box.cex<-0.5
	boxRadx<-function(st){
		rad<-strwidth(paste(" ",st," ",sep=""),units="user",cex=box.cex)/2
		pmax(rad,rep(0.03,length(rad)))
	}
	diagram::openplotmat(main = "", xlim =c(0,1), ylim = c(0,1)) #so have plot open to determine string width
	Layer1Names<-c("clusterFunction","subsample", "sequential")
	Layer1Options<-list("clusterFunction"=c("tight", "hierarchical", "pam", "kmeans"), "subsample"=c("subsample.FALSE","subsample.TRUE"), "sequential"=c("sequential.FALSE","sequential.TRUE"))
	
	Layer2Names<-c("clusterDArgs","subsampleArgs","seqArgs")
	Layer2NamesSimple<-c("clusterDArgs","subsampleArgs","seqArgs")
	#x,y,box.size,box.prop,box.type
	makeDf<-function(x,y,box.size,box.prop,box.type){data.frame(x=x,y=y,box.size=box.size,box.prop=box.prop,box.type=box.type,stringsAsFactors =FALSE)}
	posMatrix<-makeDf(x=0.5,y=0.95,box.size=0.06,box.prop=0.4,box.type="diamond")
	names<-c("clusterSingle")
	
	values<-"ellipse"
	options<-"rect"
	Layer1Pos<-list("clusterFunction"=makeDf(0.25, 0.77,boxRadx("clusterFunction"),0.4,options),"subsample"=makeDf(0.50,0.77,0.06,0.4,options), "sequential"=makeDf(0.75, 0.77,0.06,0.4,options))
	Layer1OptionsPos<-list(
		"clusterFunction"=list(
			"tight"=makeDf( 0.081, 0.59,boxRadx("tight"),0.4,values), "hierarchical"=makeDf(0.162, 0.59,boxRadx("hierarchical"),0.4,values), 
			"pam"=makeDf(0.243, 0.59,boxRadx("pam"),0.4,values), "kmeans"=makeDf(0.324, 0.59,boxRadx("kmeans"),0.4,values)), 
		"subsample"=list("subsample.FALSE"=makeDf(0.45, 0.59,0.03,0.4,values),"subsample.TRUE"=makeDf(0.55, 0.59,0.03,0.4,values)), 
		"sequential"=list("sequential.FALSE"=makeDf(0.77, 0.59,0.03,0.4,values),"sequential.TRUE"=makeDf(0.88, 0.59,0.03,0.4,values))
		)
	Layer2Pos<-list("clusterDArgs"=makeDf(0.2,0.44,boxRadx(Layer2Names[1]),0.4,options), 
		"subsampleArgs"=makeDf(0.5,.44,boxRadx(Layer2Names[2]),0.4,options), 
		"seqArgs"=makeDf(0.825, 0.44,boxRadx(Layer2Names[3]),0.4,options))
	
		#remove Layer2 if not used
	if(any(!names(Layer2Options) %in% Layer2NamesSimple)) warning("some names of the Layer2Options list are not valid")
	Layer2Options<-Layer2Options[names(Layer2Options) %in% Layer2NamesSimple]
	if(Layer2OnlyIfUsed) {
		Layer2Names<-Layer2Names[Layer2NamesSimple %in% names(Layer2Options)]
		Layer2Pos<-Layer2Pos[names(Layer2Pos)%in%names(Layer2Options)]
	}

	posMatrix<-rbind(posMatrix,do.call("rbind",Layer1Pos))
	names<-c(names,Layer1Names)
	if(plotOptions){
		posMatrix<-rbind(posMatrix,do.call("rbind",unlist(Layer1OptionsPos,recursive=FALSE)))
		names<-c(names,unlist(Layer1Options))
	}
	if(plotLayer2){
		posMatrix<-rbind(posMatrix,do.call("rbind",Layer2Pos))
		names<-c(names,Layer2Names)
	}
	posMatrix$boxCol="black"
	if(!is.null(highlight)){
		if(any(!highlight %in% names)) warning("some highlight values not matching node names and will be ignored")
		highlight<-highlight[highlight %in% names]
		m<-match(highlight,names)
		posMatrix$boxCol[m]<-"red"
	}
	if(length(Layer2Options)>0){
		Layer2OptionsPos<-lapply(names(Layer2Options),function(nam){
			opts<-Layer2Options[[nam]]
			ys<-0.44-cumsum(rep(0.05,length(opts)))
			 data.frame(x=Layer2Pos[[nam]][,"x"], y=ys, box.size=boxRadx(opts),box.prop=0.4,box.type=options,boxCol="red")
		})
		posMatrix<-rbind(posMatrix,do.call(rbind,Layer2OptionsPos))
		names<-c(names,unlist(Layer2Options))
	}



	#square coefficient matrix, specifying the links (rows=to, cols=from)
	M<-matrix("0",nrow=length(names), ncol=length(names))
	rownames(M)<-names
	colnames(M)<-names
	
	M["clusterSingle",Layer1Names]<-""
	if(plotOptions) lapply(Layer1Names,function(x){M[x,Layer1Options[[x]] ]<<-""})
		# if(2 %in% LayersToPlot){
	# 	M["clusterSingle",Layer2Names]<-""
	# }
	# lapply(names(Layer2Names),function(x){
# 		opts<-Layer1Options[[x]]
# 		if("TRUE" %in% opts) opts<-"TRUE"
# 		M[opts,Layer2Names[[x]] ]<<-1
# 	})

#	print(M)
	names<-sapply(strsplit(names,"[.]"),function(x){if(length(x)>1) x[2] else x[1]})
	par(mar=c(1, 1, 1, 1))
	transY<-posMatrix[,"y"]-(min(posMatrix[,"y"])-0.02)
	transY<-transY/(max(transY)+0.02)
	diagram::plotmat(M, pos = cbind(posMatrix[,c("x")],transY), name = names, absent="0",lwd = 1, box.lwd = 2, box.cex = box.cex, box.size = posMatrix[,"box.size"], box.type = posMatrix[,"box.type"], box.prop = posMatrix[,"box.prop"], curve = 0, main="", arr.type="triangle", arr.width=0, arr.length=0, box.lcol=posMatrix[,"boxCol"],add=TRUE)

}
```

# Introduction

The goal of this package is to encourage the user to try many different clustering algorithms in one package structure. We give tools for running many different clusterings and choices of parameters. We also provide visualization to compare many different clusterings and algorithm tools to find common shared clustering patterns. We implement common post-processing steps unrelated to the specific clustering algorithm (e.g. subsampling the data for stability, finding biomarkers related the clustering). 

We also provide a class `clusterExperiment` that inherits from `SummarizedExperiment` class to store the many clusterings and related information.  

The results from our methods will make an object `ClusterExperiment` that expands the `SummarizedExperiment` object, meaning the commands for accessing the data of a `SummarizedExperiment` will carry over to the `ClusterExperiment`.

All of our methods also have a barebones version that allows input of matrices and greater control. This comes at the expense of the user having to manage and keep track of the clusters, input data, transformation of the data, etc. We focus in this tutorial on using `SummarizedExperiment` object as the input and working with the resulting `ClusterExperiment` object. 


## Basic overview of the clustering workflow

The package encodes many common practices that are shared across clustering algorithms, like subsampling the data, computing silhouete width, sequential clustering procedures, and so forth. We describe here the basic expected usage of the package. The more technical documentation in XXX also describes in more detail additional functionality that is available that allows the user to specify the underlying clustering algorithm and greater control of the underlying parameters. 

The basic clustering workflow is 
* Implement many different clusterings using different choices of parameters using the function `clusterMany`. This results in a large collection of clusterings, where each clustering is based on different parameters. 
* Find a unifying clustering across these many clusterings using the `combineMany` function. 
* Determine whether some clusters should be merged together into larger clusters. This involves two steps:
  ** Find a hierarchical clustering of the cluster found by `combineMany` using `makeDendrogram`
  ** Merge together clusters of this hierarchy based on how the percentage of differential expression.

The basic premise of our workflow is to find small, robust clusters of samples, and then unifying them into larger clusters as relevant. We find that many algorithmic methods for choosing the appropriate number of clusters for methods  err on the side of a few number of clusters. However, we find in practice that we tend to prefer to err on finding many clusters and then merging them based on examining the data.

There is special functionality to assist in quickly visualizing and managing the results of this workflow.

## Finding related genes

A common practice after determining clusters is to perform differential gene expression analysis in order to find genes that show the greatest differences amongst the clusters. We would stress that this is purely an exploratory technique, and any p-values that result from this analysis are not valid, in the sense that they are likely to be inflated. This is because the same data was used to define the clusters and to perform differential expression analysis.

Since this is a common task, we provide the function `getBestFeatures` to perform various kinds of differential expression analysis between the clusters. A common F-statistic between groups can be chosen. However, we find that it is far more informative to do pairwise comparisons between clusters, or one cluster against all, in order to find genes that are specific to a particular cluster. An option for all of these choices is provided in the `getBestFeatures` function. The `getBestFeatures` function uses the DE analysis provided by the `limma` package. In addition, the `getBestFeatures` function provides an option to do use the "voom" correction in the `limma` package to account for the mean-variance relationship that is common in count data. Unlike edgeR or DESeq, the voom correction does not require a count matrix, and therefore can be used on FPKM or TPM entries, as well as normalized data.

## Visualization

We provide a visualization to compare many clusterings of the same data in the function `plotClusters`.  We also provide some functions that are wrappers for common visualization tasks in clustering gene expression data. We provide an heatmap function, `plotHeatmap` that is an interface to the `aheatmap` function in `NMF` package. See below for detail on the `plotHeatmap` function.

# Quickstart

We will go quickly through the standard steps of clustering using the  `clusterExperiment` package, before turning to more details. The standard workflow is the following:

* `clusterMany` -- run desired clusterings
* `combineMany` -- get a unified clustering
* `makeDendrogram` -- get a hierarchical relationship between the clusters
* `mergeClusters` -- merge together clusters with little DE between their genes.


## Data example

We will make use of a single cell sequencing experiment made available in the `scRNAseq` package at github/drisso. Note that because of the large size of the data files, you need to have the large file system installed (https://git-lfs.github.com/) to download the data. 

```{r}
library(scRNAseq)
data("fluidigm")
```

We will use the `fluidigm` dataset (see `help("fluidigm")`). This dataset is stored as a SummarizedExperiment object. We can access the data with `assay` and metadata on the samples with `colData`. 

```{r}
assay(fluidigm)[1:5,1:10]
colData(fluidigm)
NCOL(fluidigm) #number of samples
```


While there are `{r NCOL(fluidigm)}` samples, there are only 65 cells, because each cell is sequenced twice as different sequencing depth. We will limit to those with high sequencing depth. We will also do some minimal filtering of the genes, removing genes without at least 100 reads across all 65 cells.

```{r}
se<-fluidigm[,colData(fluidigm)[,"Coverage_Type"]=="High"]
whZero<-which(rowSums(assay(se))==100)
whLow<-which(rowSums(assay(se))<100)
se<-se[-whLow,]
dim(se)
```

This removed `r length(whZero)` genes out of `r NROW(fluidigm)` (`r length(whZero)` of which had zero in all samples). We now have `r NROW(se)` genes (or features) remaining.



## Step 1: Clustering with `clusterMany`

`clusterMany` lets the user quickly pick between many clustering options and run all of the clusterings in one single command. In the quick start we pick a simple set of clusterings based on varying the dimensionality reduction options. The way to pick options to vary is to give multiple values to an argument. Below in our call to `clusterMany` we will 
* set `dimReduce=c("PCA", "mostVar")` meaning cluster after a dimensionality reduction to the top principal componetns, and also cluster after filtering to the top most variable genes
* For PCA reduction, choose 5,15, and 50 principal components ifor the reduced data set (set `nPCADims=c(5,15,50)`)
* For most variable reduction, choose 100, 500, and 1000 most variable genes (set `nVarDims=c(100,500,1000)`)

Other possible options we will keep fixed to reduce the number of clusterings by giving only a single value to the argument: 
* we will use 'pam' (`clusterFunction`) 
* we will have the algorithm pick the best $k$ ( from 2-10) via silhouette distance (`findBestK=TRUE` and `ks=2:10`)
* we will unassign samples from clusters if they have negative silhouette scores (`removeSil=TRUE`). Such unassigned samples will be given the value -1. 

We will also set `isCount=TRUE` to indicate that our input data are counts. This means that operations for clustering and visualizations will internally transform the data as $log(x+1)$ (We could have alternatively explicitly set a transformation by giving a function to the  `transFun` argument, for example if we wanted $\sqrt(x)$ or $log(x+\epsilon)$ or just `function(x){x}`). 

```{r clusterMany}
ce<-clusterMany(se, clusterFunction="pam",ks=2:10,findBestK=TRUE,removeSil=c(TRUE), isCount=TRUE,dimReduce=c("PCA","mostVar"),nVarDims=c(100,500,1000),nPCADims=c(5,15,50),run=TRUE)
```



We would like to visualize this clustering using the `plotClusters` command.

```{r plotClusterEx1}
par(mar=c(1.1,7.1,4.1,1.1))
plotClusters(ce,main="Clusters from clusterMany", whichClusters="workflow",axisLine=-1)
```

This plot shows the samples in the columns, and different clusterings on the rows. Each sample is color coded based on its clustering for that row, where the colors have been choosen to try to match up clusters across different clusterings that show large overlap. Moreover, the samples have been ordered so that each subsequent clustering (starting at the top and going down) will try to order the samples to keep the clusters together, without rearranging the clustering blocks of the previous clustering/row.

We have set `whichClusters="workflow"` to only plot clusters created from the workflow. There are many different options for how to run `plotClusters` discussed in Section XXX, but for now, this plot is good enough for a quick visualization. 

### The output 

The output of `clusterMany` is a `ClusterExperiment` object. This is a class built for this package and explained in Section XXX. In the object `ce` the clusters are stored, names and colors for each cluster within a clustering are assigned, and other information about the clustersins. Furthermore, all of the information in the original `SummarizedExperiment` are retained. 

There are many accessor functions that help you get at the information in a `ClusterExperiment` object and some of the most relevant are described in Section XXX. (`ClusterExperiment` objects are S4 objects, and are not lists). 

For right now we will only mention the most basic such function that retrieves the actual cluster assignments. This is the `clusterMatrix` function, that returns a matrix where the columns are the different clusterings and the rows are samples. Within a clustering, the clusters are encoded by consecutive integers. 

```{r clusterMatrix}
head(clusterMatrix(ce))
```

Notice that some of the samples are assigned the value of -1. -1 has the significance of encoding samples that are not assigned to any cluster. Why this is the case depends on the underlying choices of the clustering routine. In this case, we chose `removeSil=TRUE` so that samples with negative silhouette distance from their originally assigned clusters are discarded. 

(Another special value is -2 discussed in Section XXX.)

## Step 2: Find a consensus with `combineMany`

To find a consensus cluster across the many different clusterings created by `clusterMany` the function `combineMany` can be used next. 

```{r}
ce<-combineMany(ce)
```

Notice we get a warning that we did not specify any clusters to combine, so it is using the default -- those with `clusterType="clusterMany"` that we got in the previous step of the workflow. 

If we look at the `clusterMatrix` of the returned `ce` object, we see that the new cluster from `combineMany` has been added to the existing clusterings. This is the basic strategy of the functions in this package. Any clustering that is created is added to existing clusterings, so the user does not need to keep track of past clusterings and can easily compare what has changed. 

```{r}
head(clusterMatrix(ce))
plotClusters(ce,whichClusters="workflow")
```

The default result of `combineMany` is not usually a great choice, and certainly isn't helpful here. The resulting clusters leave most samples unassigned. This is because the default way of combining is very conservative -- it require samples to only be in the same cluster if they are in the same clusters in *every clustering*, and makes clusters that satisfy this requirement. This is quite stringent. We can vary this by setting `proportion` argument to indicate the minimum proportion of times they should be together with other samples in the cluster they are assigned to. Explicit details on how this is done will be discussed  in Section XXX.

```{r}
ce<-combineMany(ce,proportion=0.7)
plotClusters(ce,whichClusters="workflow")
```


We can also visualize the proportion of times these clusters were together across these clusterings (this information was made and stored in the ClusterExperiment object when we called `combineMany` as long as `proportion` value is <1):

```{r}
plotCoClustering(ce)
```

## Step 3: Merge clusters together with `makeDendrogram` and `mergeClusters`

For computational simplicity in the vignette, we have not created many clusters with `clusterMany`. However, it is not uncommon in practice to create forty or more clusterings with `clusterMany`. In which case, the results of `combineMany` can often still result in too many small clusters. We might wonder if they are necessary or could be logically combined together. We could change the value of `proportion` in our call to `combineMany`. But we have found that it is often after looking at the clusters and how different they look on individual genes that we best make this determination, rather than the proportion of times they are together in different clustering routines. 

For this reason, we often find the need for an additional clustering step that merges clusters together that are not different, based on running tests of differential expression between the clusters found in `combineMany`. We often display and use both sets of clusterings side-by-side (that from `combineMany` and that from `mergeClusters`)

`mergeClusters` needs a hierarchical clustering of the clusters. With this hiearchy it then goes progressively up that hierarchy, deciding whether two adjacent clusters can be merged. The function `makeDendrogram` makes such a hierarchy between clusters (by applying `hclust` to the mediods of the clusters). Because the results of `mergeClusters` are so dependent on that hierarchy, we require the user to call `makeDendrogram` rather than calling it internally. This is because different options in `makeDendrogram` can affect how the clusters are hierarchically clustered, and we want to encourage the user make these choices.

```{r mergeClusters}
ce<-makeDendrogram(ce,dimReduce="mostVar",ndims=500)
ce<-mergeClusters(ce)
plotClusters(ce,whichClusters="workflow")
```

In this case, merging the clusters had no effect -- no clusters were identified as needing to merged into a larger cluster.

Finally, we can do a heatmap visualizing our final clustering (see Section XXX for many more options to `plotHeatmap`)

```{r plotHeatmap}
plotHeatmap(ce,clusterSamplesData="dendrogramValue")
```

# ClusterExperiment Objects

The `ce` object that we created from calling `clusterMany` is a `ClusterExperiment` object. The `ClusterExperiment` class is used by this package to keep the data and the clusterings together. It inherits from `SummarizedExperiment`, which means the data and `colData` and other information orginally in the `fluidigm` object is retained and can be accessed with the same functions as before. The `ClusterExperiment` object additionally stores clusterings and information about the clusterings along side the data. This helps keep everything together, and like the `SummarizedExperiment` object, allows for simple things like subsetting to a reduced set of subjects and being confident that the corresponding clusterings, colData, and so forth are similarly subset.

Typing the name at the control prompt results in a quick summary of the object. 

```{r show}
ce
```

This summary tells us the total number of clusterings (`r nClusters(ce)`), and gives some indication as to what parts of the standard workflow have been completed and stored in this object. It also gives information regarding the `primaryCluster` of the object. The `primaryCluster` is just one of the clusters that has been choosen to be the "primary" cluster, meaning that by default various functions will turn to this clustering as the desired cluster to use. The "primaryCluster" can be reset by the user (see `primaryClusterIndex`). `clusterMany` arbitrarily sets the 'primaryCluster' to the first one, and each later step of the workflow sets the primary index to the most recent, but the user can select a different one.

There are also additional commands to access the clusterings and their related information (type `help("ClusterExperiment-methods")` for more).

The cluster assignments are stored in the `clusterMatrix` slot of `ce`, with samples on the rows and different clusterings on the columns. The different clusters are stored as consecutive integers, with '-1' and '-2' having special meaning. '-1' refers to samples that were not clustered by the clustering algorithm. In our example, we removed samples with negative silhouette distance to their cluster, so they were assigned '-1'.

```{r CEHelperCommands1}
head(clusterMatrix(ce))
primaryCluster(ce)
```

`clusterLabels` gives the column names of the `clusterMatrix`; `clusterMany` has given column names based on the parameter choices, and later steps in the workflow also give a name. But clusterings might also have no specific label if the user created them. `clusterType` on the otherhand indicates what call made the clustering. 

```{r CEHelperCommands2}
clusterLabels(ce)
clusterType(ce)
```

The information that was in the original `fluidigm` object has also been preserved, like `colData` that contains information on each sample.

```{r SECommandsOnCE}
colData(ce)[,1:5]
```

Another important slot in the `ClusterExperiment` object is the `clusterLegend` slot. This consists of a list, one element per column or clustering of `clusterMatrix`. 

```{r CEClusterLengend}
clusterLegend(ce)
```

We can see that each element of `clusterLegend` consists of a matrix, with number of rows equal to the number of clusters in the clustering. The columns store information about that cluster. `clusterIds` is the internal id used in `clusterMatrix` to identify the cluster, `name` is a name for the cluster, and `color` is a color for that cluster. `color` is used in plotting and visualizing the clusters, and `name` is an arbitrary character string for a cluster. They are automatically given default values when the `ClusterExperiment` object is created, but we will see under the description of visualization methods how the user might want to manipulate these for better plotting results. 


# Visualizing the data

## Plotting the clusters
We demonstrated during the quick start that we can visualize multiple clusterings using the `plotClusters` command.

```{r plotClusterEx1}
par(mar=c(1.1,7.1,4.1,1.1))
plotClusters(ce,main="Clusters from clusterMany",whichClusters="workflow",axisLine=-1)
```

We can assign new labels if we prefer, for example to be more succinct by changing the clusterLabels of the object (note we are permanently changing the labels here within the `ClusterExperiment` object) 

```{r plotClusterEx1_nicelabels}
cl<-clusterLabels(ce)
cl<-gsub("Features","",cl)
clusterLabels(ce)<-cl
par(mar=c(1.1,7.1,4.1,1.1))
plotClusters(ce,main="Clusters from clusterMany",axisLine=-1)
```

We can get very different plots depending on how we order the clusterings. The argument `whichClusters` allows you to give choose different clusters or provide an explicit ordering of the clusters. `whichClusters` can take either a single character value, or a vector of either characters  or indices. If `whichClusters` matches either "all" or "workflow", then the clusters chosen are either all clusters, or only clusters from the most recent calls to the workflow function. Choosing "workflow" removes both user-defined clusters but also previous calls to the workflow that have since been rerun, but the old clusterings have not been erased, see Section XXX. Setting `whichClusters` can be a useful if you have called a method like `combineMany` several times, as we did, only with different parameters. All of those runs are saved (unless `eraseOld=TRUE`), but you may not want to plot them.  Other than these choices, if `whichClusters` is a character vector, the entries should match a cluster type (like `clusterMany`). Alternatively, the user can specify specific numeric indices corresponding to the columns of `clusterMatrix` that provide the order of the clusters.

```{r plotClusterEx1_newOrder}
par(mar=c(1.1,7.1,4.1,1.1))
plotClusters(ce,whichClusters="clusterMany",
               main="Clusters from clusterMany, different order",axisLine=-1)
```

We can also add to our plot (categorical) information on each of our subjects from the `colData` of our fluidigm object (which is also retained in our ClusterExperiment object). This can be helpful to see if the clusterings correspond to other features of the samples, such as sample batches. 

```{r plotClusterEx1_addData}
par(mar=c(1.1,10.1,4.1,1.1))
plotClusters(ce,whichClusters="workflow", sampleData=c("Biological_Condition","Cluster2"), 
               main="Clusters from clusterMany, different order",axisLine=-1)
```

### Saving the alignment of plotClusters

`plotClusters` invisibly returns a `ClusterExperiment` object. In our earlier calls to `plotCluster`, this would be the same as the input object and so there is no reason to save it. However, the alignment and color assignments created by `plotClusters` can be requested to be saved via the `resetNames`, `resetColors` and `resetOrderSamples` arguments. If any of these are set to TRUE, then the object returned will be different than those of the input. Specifically, if `resetColors=TRUE` the `colorLegend` of the returned object will be changed so that the colors assigned to each cluster will be as were shown in the plot. Similarly, if `resetNames=TRUE` the names of the clusters will be changed to be integer values, but now the integers will be aligned to try to be the same across clusters (and therefore not consecutive integers, which is why these are saved as names for the clusters and not `clusterIds`). If `resetOrderSamples=TRUE`, then the order of the samples shown in the plot will be similarly saved in the slot `orderSamples`. 

```{r plotClusterEx1_setColors}
par(mar=c(1.1,10.1,4.1,1.1))
ce<-plotClusters(ce,whichClusters="workflow", sampleData=c("Biological_Condition","Cluster2"), 
               main="Clusters from clusterMany, different order",axisLine=-1,resetNames=TRUE,resetColors=TRUE,resetOrderSamples=TRUE)
clusterLegend(ce)[2:3]
```

The `clusterLegend` slot of the object no longer has the default color/name assignments, but now has names and colors that match across the clusters. 

We can also force `plotClusters` to use the existing color definitions, rather than create its own. This makes particular sense if you want to have continuity between plots -- i.e. be sure that a particular cluster always has a certain color -- but would like to do different variations of plotClusters to get a sense of how similar the clusters are.

For example, I set the colors above based on the cluster order from `plotClusters` where the clusters were ordered according to the workflow. But now I want to plot only the clusters from `clusterMany`, yet keep the same colors as before so I can compare them. I do this by setting the argument `existingColors="all"`, meaning use all of the existing colors (currently this is the only option available for how to use the existing colors).

```{r plotClusterEx1_setColors}
par(mar=c(1.1,10.1,4.1,1.1))
par(mfrow=c(1,2))
plotClusters(ce, sampleData=c("Biological_Condition","Cluster2"), whichClusters="workflow", existingColors="all",
               main="Workflow Clusters, fix the color of clusters",axisLine=-1)

plotClusters(ce, sampleData=c("Biological_Condition","Cluster2"), existingColors="all",
             whichClusters="clusterMany",
               main="clusterMany Clusters, fix the color of clusters",axisLine=-1)
```

## Heatmap with the clusters
There is also a default heatmap command for a ClusterExperiment object that we used in the Quick Start. By default it clusters on the most variable features (after transforming the data) and shows the primaryCluster alongside the data. The primaryCluster now that we've run the workflow will be set as that from the mergeClusters step. 

```{r plotHeatmap_Ex1}
plotHeatmap(ce,main="Heatmap with clusterMany")
```



The `plotHeatmap` command has numerous options, in addition to those of `aheatmap`. `plotHeatmap` mainly provides additional functionality in the following areas: 

* Easy inclusion of clustering information or sample information, based on the ClusterExperiment object
* Additional methods for ordering/clustering the samples that makes use of the clustering information
* Use separate input data for clustering and for visualization

### Displaying clustering or sample information

Like `plotClusters`, `plotHeatmap` has a `whichClusters` option that behaves similarly to that of `plotClusters`.  In addition to the options `all` and `workflow` that we saw with `plotClusters`, `plotHeatmap` also takes the option `none` (no clusters shown) and `primary` (only the primaryCluster). The user can also request a subset of the clusters by giving specific indices to `whichClusters` like in `plotClusters`.

```{r plotHeatmap_Ex1.1}
plotHeatmap(ce,whichClusters="workflow",main="Heatmap with clusterMany, all clusters",annLegend=FALSE)
```

Notice I also passed the option 'annLegend=FALSE' to the underlying `aheatmap` command (with many clusterings shown, it is often not useful to have a legend for all the clusters because the legend doesn't fit on the page!). The many detailed commands of `aheatmap` that are not set internally by `plotHeatmap` can be passed along as well. 

Like `plotClusters`, `plotHeatmap` takes an argument `sampleData`, which refers to columns of the `colData` of that object and can be included.  

### Additional options for clustering/ordering samples

I can choose to instead to not cluster the samples, but order them by cluster:

```{r plotHeatmap_Ex1.2}
plotHeatmap(ce,clusterSamplesData="primaryCluster",whichClusters="all",main="Heatmap with clusterMany",annLegend=FALSE)
```

As an improvement upone this, I can cluster my clusters into a dendrogram so that the most similar clusters will be near each other. I can do this by explicitly calling `makeDendrogram` (and therefore be able to control the choices in how it is done, which we will discuss below) which will store a dendrogram in our clusterExperiment object; otherwise `plotHeatmap` will call makeDendrogram with its default settings. I will also add some data about my samples from my `colData` slot using the `sampleData` argument. 

```{r plotHeatmap_Ex1.2}
plotHeatmap(ce,clusterSamplesData="dendrogramValue",whichClusters="primaryCluster",main="Heatmap with clusterMany",sampleData=c("Biological_Condition","Cluster2"),annLegend=FALSE)
```

### Using separate input data for clustering and for visualization

**To Add**

### Plotting coClusterings

# The clustering workflow

We will now go into more detail about important options for the main parts of the clustering workflow.

## clusterMany

### Choosing options for `clusterMany`
In the quick start section we picked some simple and familiar clustering options that would run quickly and needed little explanation. However, our workflow generally assumes more complex options and more parameter variations are tried. Before getting into the options of `clusterMany`, let us first describe some of these more complicated setups, since many of the parameters dependon understanding them. 

Clustering algorithms generally start with a particular predefined distance or dissimilarity between the samples, usually encoded in a $n x n$ matrix, $D$. The input could also be similarities, though we will continue to call such a matrix $D$. In our package, we consider only such clustering algorithms. 

The simplest scenarios is a there is a simple calculation of $D$ and then a clustering of $D$, usually dependent on particular parameters.  In this package we try to group together algorithms  based on common parameters and operations that can be done. The help files of `clusterD` document these choices more fully, but we give an overview here.

Currently there are two "types" of algorithms we consider. One type are called "K" algorithms because their main parameter requirement is that the user specifies the number of clusters ($K$) to be created. They assume the input $D$ are dissimilarities, but have no other expectation of $D$. "pam" and "kmeans" are examples of such types of algorithms. 

Another type of algorithm we call "01" algorithms, because the algorithm assumes that the input are similarities between samples and that the similarities encoded in $D$ are on a scale of 0-1. They use this fact to make the user specify be not the number of final clusters, but a measure of similarity $\alpha$ required for samples in the same cluster (on a scale of 0-1). Given $\alpha$, the algorithm then has a method to then determine the clusters (so $\alpha$ implicitly determines $K$). These methods rely on the assumption that because the 0-1 scale has special significance, the user will be able to make an determination more easily as to the level of similarity need to be a true cluster. The current 01 methods are `tight` and `hierarchical` *XXXNeed to change this*. 

In addition to the basic clustering algorithms on a matrix $D$, we also implement many other common cluster processing steps that arerelevant to the result of the clustering. We have already seen such an example with dimensionality reduction, where the imput $D$ is determined based on different distances on the data. A more significant processing that we perform is calculation of a dissimilarity matrix $D$ not by a distance on the vector of data, but by subsampling and clustering of the subsampled data. The resulting $D$ matrix is a matrix of co-Clustering percentages. Each entry is a percentage of time the two samples shared a clustering over the many subsamplings of the data (there are slight variations as how this  can be calculated, see help of `subsampleClustering` ). Note that this implies there actually two different clustering algorithms (and sets of corresponding parameters) -- one for the clustering on the subsampled data, and one for the clustering of the resulting $D$ of the percentage of coClustering of samples. `clusterMany` focuses on varying parameters related to the clustering of $D$, and generally assumes that the underlying clustering on the subsampled data is simple (e.g. "pam"). (The underlying clustering machinery in our package is performed by a function `clusterSingle` which is not discussed in this tutorial, but can be called explicitly for fine-grained control over a single clustering). The subsampling option is computationally expensive, and when coupled with comparing many parameters, does result in a length evaluation of `clusterMany`. However, we recommend it as one of the most useful methods for getting stable results.

Another complicated addition to the clustering that requires additional explanation is the implementation of sequential clustering. This refers to clustering of the data, then removing the "best" cluster, and then re-clustering the remaining samples, and then continuing this iteration until all samples are clustered (or the algorithm in some other way calls a stop). Such sequential clustering can often be convenient when there is very dominant cluster, for example, that is far away from the other mass of data. Removing samples in these clusters and resampling can sometimes be more productive and result in results more robust to the choice of samples. Because of the iterative nature of this process, there are many possible parameters (see `help(seqCluster)`). `clusterMany` does not allow variation of very many of these parameters, but instead just has the choice of running the sequential clustering or not. Paired with subsampling to determine $D$ at each step, sequential clustering can also be quite computationally expensive.

### Specific parameters of `clusterMany`
Now that we've explained the underlying architecture of the clustering provided in the package, we give a list of the parameters that can be varied and the options. More details about some of these parameters will be given below. Note these are not all the arguments of `clusterMany` but are the only ones that can be given multiple options (meaning that the combinations of these will be tried)

* `clusterFunction` The clustering function to be used. If `subsample=TRUE`, then is the method that will be used on the co-clustering matrix $D$ created from subsampling the data (and using pam clustering on the subsampled data). Otherwise, it is the clustering method that will be used on  `dist(t(x))`
* `ks` The argument 'ks' is interpreted differently for different choices of the other parameters. If `sequential=TRUE` is part of the combination, `ks` defines the argument `k0` of sequential clustering (see `help(seqCluster)`), which is approximately like the initial starting point for the number of clusters.  When/if `clusterFunction="pam"` and `findBestK=TRUE`, `ks` defines the range of values to search for the best k (see the details in `help(clusterMany)` for more). If the clustering method is a 01
* sequential
* subsample
* dimReduce
  * nVarDims
  * nPCADims
* alphas
* findBestK
* removeSil
  * silCutoff



`clusterMany` tries to have generally simple interface, and for this reason makes choices about what is meant by certain combinations. For example, above when we set `findBestK=TRUE` that implies that `ks=2:10` means that the clustering should find the best $k$ out of the range of 2-10. However, in other cases `ks` might indicate the specific number of clusters, $k$ that should be found. 

Here is a simple example where we want to basic PAM clustering for different k (we choose this for simplicity, but will discuss better, more computationally expensive, clustering options below). We also can compare if we decide to treat samples with negative silhouette values as unassigned or not. `compareMany` computes all of these clusterings based on one command. Each combination of the arguments that is given will result in another clustering. So, by putting both TRUE and FALSE for removeSil, it that all combations of 'k' and 'removeSil' will be tried. 

We will also compare different dimensionality reductions: the top 5,15, and 50 principal components of the data as well as the top 100, 500, and 1000 most variable genes. 

### Dealing with large numbers of clusterings

A good first check before running `clusterMany` is to determine how many  clusterings you are asking for. `clusterMany` has some limited internal checks to not do unnecessary duplicates (e.g. `removeSil` only works with some clusterFunctions so `clusterMany` would detect that), but generally takes all combinations. This can take a while for more complicated clustering techniques, so it is a good idea to check what you are getting into. You can do this by running `clusterMany` with `run=FALSE`

In the following we expand our 

```{r clusterManyCheckParam}
checkParam<-clusterMany(se, clusterFunction="pam", ks=2:10,removeSil=c(TRUE,FALSE), isCount=TRUE,dimReduce=c("PCA","mostVar"),nVarDims=c(100,500,1000),nPCADims=c(5,15,50),run=FALSE)
dim(checkParam$paramMatrix) #number of rows is the number of clusterings
```

Each row of this matrix is a requested clustering (the columns indicate the value of a possible parameter). Our selections indicate `r nrow(checkParam$paramMatrix)` different clusterings. 

The output can also be fed back into clusterMany [**Check this**], which allows the user to modify the input (though do so carefully, there are not robust checks as to whether the input is of the right form). The best reason for modifying this would be to remove some of the clusterings. For example, if you only want to do some of the combinations, you could remove the unwanted ones by removing those rows of the `paramMatrix`.

Here is a simple example where we want to basic PAM clustering for different k (we choose this for simplicity, but will discuss better, more computationally expensive, clustering options below). We also can compare if we decide to treat samples with negative silhouette values as unassigned or not. `compareMany` computes all of these clusterings based on one command. Each combination of the arguments that is given will result in another clustering. So, by putting both TRUE and FALSE for removeSil, it that all combations of 'k' and 'removeSil' will be tried. We will also set `isCount=TRUE` to indicate that our input data are counts. This means that operations for clustering and visualizations will transform the data as $log(x+1)$ (We could have alternatively explicitly set a transformation by giving a function to the  `transFun` argument, for example if we wanted $\sqrt(x)$ or $log(x+\epsilon)$). 

We will also compare different dimensionality reductions: the top 5,15, and 50 principal components of the data as well as the top 100, 500, and 1000 most variable genes. 


**Tip** A good first check before running `clusterMany` is to determine how many such clusterings you are asking for, by setting `run=FALSE`. `clusterMany` has some limited internal checks to not do unnecessary duplicates (e.g. `removeSil` only works with some clusterFunctions so `clusterMany` would detect that), but generally takes all combinations. This can take a while for more complicated clustering techniques, so it is a good idea to check what you are getting into. 

```{r clusterManyCheckParam}
checkParam<-clusterMany(se, clusterFunction="pam", ks=2:10,removeSil=c(TRUE,FALSE), isCount=TRUE,dimReduce=c("PCA","mostVar"),nVarDims=c(100,500,1000),nPCADims=c(5,15,50),run=FALSE)
dim(checkParam$paramMatrix) #number of rows is the number of clusterings
```

Each row of this matrix is a requested clustering (the columns indicate the value of a possible parameter). Our selections indicate `r nrow(checkParam$paramMatrix)` different clusterings. 

The output can also be fed back into clusterMany [**Check this**], which allows the user to modify the input (though do so carefully, there are not robust checks as to whether the input is of the right form). The best reason for modifying this would be to remove some of the clusterings. For example, if you only want to do some of the combinations, you could remove the unwanted ones by removing those rows of the `paramMatrix`.

In the interest of time, we will scale back the number of clusterings by picking letting `clusterMany` find the best $K$ via total silhouette distance. 

```{r clusterManyCheckParam2}
checkParam<-clusterMany(se, clusterFunction="pam",ks=2:10,findBestK=TRUE,removeSil=c(TRUE), isCount=TRUE,dimReduce=c("PCA","mostVar"),nVarDims=c(100,500,1000),nPCADims=c(5,15,50),run=FALSE)
dim(checkParam$paramMatrix) #number of rows is the number of clusterings

```

We can now run this, either by giving the information in `checkParam$paramMatrix` to clusterMany argument `paramMatrix`, or by recalling the function from scratch. There's no advantage in giving `paramMatrix` rather than just recalling `clusterMany`, but we'll do it here just to show how it is done. 

```{r clusterManyCheckParam2}
# ce<-clusterMany(se,  paramMatrix=checkParam$paramMatrix, clusterDArgs=checkParam$clusterDArgs, seqArgs=checkParam$seqArgs,subsampleArgs=checkParam$subsampleArgs)
ce<-clusterMany(se, clusterFunction="pam",ks=2:10,findBestK=TRUE,removeSil=c(TRUE), isCount=TRUE,dimReduce=c("PCA","mostVar"),nVarDims=c(100,500,1000),nPCADims=c(5,15,50),run=TRUE)
```

Note that we also provide additional arguments `clusterDArgs`, `seqArgs` and `subsampleArgs` which normally we might neglect with a direct call to `clusterMany`. This is because in creating the `paramMatrix`, `clusterMany` may internally change these default values, and we want to make sure we exactly replicate what we would get from a direct call


## combineMany 

It can be useful to create heatmaps that keep the clusters together. However, arbitrarily ordered clusters will not make compelling heatmaps either. The function `clusterHclust` will perform hierarchical clustering of the cluster mediods, and provide a dendrogram that will order the samples according to this clustering of the clusters.

```{r clusterHclust, warning=FALSE}
clDendro<-clusterHclust(dataMatrix,cl$clustering,full=FALSE)
plot(clDendro)
```

More useful, however, is to use this in conjunction with `plotHeatmap`, in which case we want the individual samples in the dendrogram, not just the clusters. Notice that `full=TRUE` is necessary to get such a dendrogram. Then we give the data as `heatData` to `plotHeatmap`, indicating that it should be used for creating the color-image of the data, but provide our dendrogram to `clusterData` to indicate it will provide the clustering of the data. 

```{r plotHeatmapHclust}
clDendroFull<-clusterHclust(dataMatrix,cl$clustering,full=TRUE,unassigned="outgroup")
plotHeatmap(cl$clustering,heatData=exp(dataMatrix),eps=0,clusterData=clDendroFull,annCol = data.frame(clusterExperiment=cluster, TrueClusters=trueCluster))
```

Notice that the unassigned samples ("-1") are chosen in `clusterHclust` to be assigned to an outgroup in the dendrogram based on the argument `unassigned`.

One could also choose to remove them, making sure that the input data matrix to `plotHeatmap` doesn't contain these samples.

```{r, warning=FALSE}
clDendroFull<-clusterHclust(dataMatrix,cl$clustering,full=TRUE,unassigned="remove")
whRm<-which(cl$clustering>0)
plotHeatmap(cl$clustering[whRm],heatData=exp(dataMatrix)[whRm,],eps=0,clusterData=clDendroFull,annCol = data.frame(clusterExperiment=cluster[whRm], TrueClusters=trueCluster[whRm]))
```

### Hierarchical Contrasts and Merging clusters

We can also perform tests based on this hierarchy of clusters. Specifically, we can test for each node of the hierarchy, whether the mean of the set of clusters to the right split of the node is equal to the mean on the left split. We do this by creating a contrast of the corresponding means and testing it for each gene via `getBestFeatures`, where the type is set to `"Dendro"` and we provide the dendrogram created by `clusterHclust` (but setting `full=FALSE` so that individual samples are not in the dendrogram).

```{r}
dendroGenes<-getBestFeatures(cl$clustering,simData,type="Dendro",p.value=0.05,number=ncol(simData),dendro=clDendro)
head(dendroGenes)
```

The output is similar to before, but now there are two identifiers for the contrast: one is `ContrastName` which gives the Node being compared, and the other is `Contrast` which is the actual mathematical statement of the mean comparison between clusters that is being performed.

We can use this basic idea to merge clusters based on what percentage of genes are found differentially expressed. The `mergeClusters` function performs such contrasts for each node, and then calculates the proportion of significant hypotheses. There are several methods implemented, but the simplest is just the proportion found by calculating the proportion of DE genes at a given False Discovery Rate thresshold (using the Benjamini-Hochberg procedure).

```{r mergeClusters}
mergeOut<-mergeClusters(dat=simCount, cl=cl$clustering, mergeMethod = c("adjP"), cutoff = 0.1, plotType = "mergeMethod",countData=TRUE)
table(Meged_clusters=mergeOut$cl,Original_clusters=cl$clustering)
print(mergeOut$propDE)
```

The function optionally creates a plot showing the dendrogram and what clusters were merged. `mergeClusters` invisibly returns information, including the merged clusters (`cl`), and a matrix of different estimates based on different methods for computing the proportion (`propDE`).

# Finding Marker Genes

The function `getBestFeatures` calls `limma` on input data to determine the gene features most associated with the found clusters. The minimal required input is the expression data and a vector of the (numeric) cluster labels. The method ignores samples in clusters marked by "-1" ("-1" always signifies samples not clustered). By default,  `getBestFeatures` finds significant genes based on a F-test between the clusters, but the option `type` can provide other measures and is the most common one to be changed by the user. 

In this demonstration, clustering will first be performed with basic PAM with k=4 and removing negative silhouette widths. We use the simulated data found in  the package `simData`.

`getBestFeatures` will be called to determine the top features associated with every pairwise comparison between the clusters. By default, `topTable` in `limma` only returns the top 10, which in this context means 10 per each of the pairwise comparisons.

```{r getBestFeatures}
data(simData)
cl = clusterSingle(simData, clusterFunction="pam", subsample=FALSE, sequential=FALSE, clusterDArgs=list(k=4,removeSil=TRUE))
pairsAll<-getBestFeatures(cl$clustering,simData,type="Pairs")

head(pairsAll)
```

The column `Contrast` in the output signifies the pairwise difference (or contrast) for which the information on the row comes from. `IndexInOriginal` matches the gene to its index in the original dataset. The other columns are provided by `topTable` in `limma` (see documentation therein).

`getBestFeatures` accepts arguments to `limma`'s function `topTable` to decide which genes should be returned (and in what order). In particular, we can set an adjusted p-value cutoff for each contrast, and set `number` to control the number of genes returned for each contrast. By setting `number` to be the length of all genes, and `p.value=0.05`, we can return all genes for each contrast that have adjusted p-values less than 0.05.

```{r getBestFeaturesAllSig}
data(simData)
cl = clusterSingle(simData, clusterFunction="pam", subsample=FALSE, sequential=FALSE, clusterDArgs=list(k=4,removeSil=TRUE))
pairsAll2<-getBestFeatures(cl$clustering,simData,type="Pairs",p.value=0.05,number=ncol(simData))
table(pairsAll2$Contrast)
```

Notice that the same genes can be replicated across different contrasts:

```{r getBestFeaturesNumber}
nrow(pairsAll2)
length(unique(pairsAll2$Gene))
```

Note that by using contrasts, we are making use of all of the data, not just the samples in the particular cluster pairs being compared. This means the variance is estimated with all the samples.

## Heatmaps of Marker Genes

One way to visualize samples with only the features found by `getBestFeatures` is to use the `plotHeatmap` function. The `plotHeatmap` function minimally requires two arguments - a vector of cluster assignments and the data matrix. `plotHeatmap` uses the heatmap function `aheatmap` in the package `NMF`, but provides an interface that is useful for the standard plots desired for evaluating the clustering output. In particular, it is designed to draw the cluster assignments of the samples on the top, where different colors denote different cluster assignments; lack of assignment, denoted by "-1", are automatically colored white.  In the code example below, `clusterData` is set to the data matrix to define the color scale. Currently, `plotHeatmap` internally transforms the input data (i.e. argument `heatData`) to the log scale; this will be fixed to be an option in future versions, but currently, passing of already logged data is not supported and log-scale data should be exponentiated for use with `plotHeatmap`.

```{r plotHeatmap}
dataMatrix = simData[, unique(pairsAll2$IndexInOriginal)]
cluster = cl$clustering
plotHeatmap(cluster, exp(dataMatrix), eps=0, clusterData=dataMatrix)
```

Additional covariates can also be plotted by giving a matrix of values to `annCol`; unlike `aheatmap`, `plotHeatmap` will assume that such values should be converted to factors unless told otherwise by the argument `whAnnCont`.

```{r}
plotHeatmap(cluster, exp(dataMatrix), eps=0, clusterData=dataMatrix, annCol = data.frame(clusterExperiment=cluster, TrueClusters=trueCluster))
```

Another feature of plotHeatmap is that a different dataset can be used for determining the clustering than for determining the color-scale image of the heatmap. This can be useful, for example, for clustering on the normalized data which is not on the count-scale, but showing the original count-scale in the color-scale which is more interpretable. 


# Fine tuned clustering with `clusterSingle`

Let us start with a walkthrough of `clusterSingle`.  `clusterSingle` is the primary function for performing the primary clustering tasks. Each of these tasks has an underlying (user-accessible) function and `clusterSingle` is a wrapper around these tasks:

* clustering of a distance matrix (`clusterD`)
* subsampling the samples, clustering the subsamples, and forming a co-occurance matrix (`subsampleClustering`)
* sequentially removing the best cluster, and reclustering the remaining clusters (`seqCluster`)

The arguments of `clusterSingle` indicate which of these tasks should be performed and allow the user to pass options for each of these tasks (that are passed to the downstream functions). The options are documented in the more detailed documentation vignette, but here we run through some common examples. 

Here are the argument calls for `clusterSingle`:
```{r clusterSingleArgs, eval=FALSE}
clusterSingle(x, subsample = TRUE, sequential = FALSE, clusterFunction = c("tight", "hierarchical", "pam", "kmeans"), clusterDArgs = NULL, subsampleArgs = NULL, seqArgs = NULL)
```

The primary arguments `clusterFunction`,`subsample`, and `sequential` govern each of these tasks:

We can visualize arguments and the values they can take on as a diagram: 

```{r visualizeClusterAll, echo=FALSE, message=FALSE,out.width="600px",out.height="600px"}
drawClusterAll()
```

The remaining  arguments (`clusterDArgs`, `subsampleArgs`, and `seqArgs`) are for passing additional options to each of these tasks, and we will show examples of how they are used. 

## Example: PAM Clustering

Let us use the function `clusterSingle` to perform simple Partition Around Medoids (PAM) clustering on our expression data set with `k=5`. In this expression matrix, rows are samples and columns are features. 

To do standard pam clustering, we set `clusterFunction="pam"`. The remaining tasks (subsampling and sequential) will not be performed, so we set `subsample=FALSE` and `sequential=FALSE`, which we can visualize as:

```{r visualizeClusterAllPam, echo=FALSE}
drawClusterAll(highlight = c("pam", "subsample.FALSE", "sequential.FALSE"))
```

However, notice that doing this will return an error:
```{r clusterPamError,error=TRUE}
simpleCluster = clusterSingle(expressions, subsample=FALSE, sequential=FALSE, clusterFunction="pam")
```

This is because even to perform simple PAM, the PAM algorithm needs additional parameters to be set, namely 'k', the number of groups. 

However, there is no argument in `clusterSingle` for this. Because it is specific to a particular choice of clustering function, we don't want to clutter up `clusterSingle` with all the possible options that might be needed downstream for a particularly task. Instead, `clusterSingle` lets you pass such arguments to the function that actually does the task of clustering a distance matrix, `clusterD` via the `clusterDArgs` option. So the user doesn't need to call `clusterD` explicitly, but often will need to pass an argument to `clusterD`.  

In this case we need to pass the argument 'k'. Specifically, we need to create a list with an element of name "k" that takes on the value, say 5, for our k.

```{r visualizePamArgs, echo=FALSE}
drawClusterAll(highlight = c("pam", "subsample.FALSE", "sequential.FALSE"),Layer2Options=list("clusterDArgs"="k"),plotOptions=TRUE)
```

```{r clusterSinglePam, warning=FALSE, message=FALSE}
library(clusterExperiment)
library(cluster)
#load("data/expressions.Rda")
expression = expressions[, 1:3]
simpleCluster = clusterSingle(expression, subsample=FALSE, sequential=FALSE, clusterFunction="pam", clusterDArgs=list('k'=5))
table(simpleCluster$clustering)
```

The result is a one-element list with a vector of cluster assignments for each sample.

Let us compare this result with the result from calling `pam` with `k=5` from the `cluster` package. 

```{r comparePam}
pamCluster = pam(dist(expression), 5)
table(clusterExperiment=simpleCluster$clustering, cluster=pamCluster$clustering)
table(clusterExperiment=simpleCluster$clustering, true=expressions$cluster)
```

We have now verified that these two function calls perform the same task. Incidentally, if we know the true number of clusters, a simple PAM procedure leads to good performances in this toy example. This is not surprising, since the clusters are simulated with Gaussian noise, in low dimensions.


### More options for PAM clustering

There are other options we can pick to change the clustering of the distance matrix. A full documentation of possible options can be found in the help files of `clusterD`. Some of the useful options associated with PAM are the following common tasks:

* Finding the best k, based on silhouette width (`findBestK`) over a range of possible K (`kRange`)
* Removing samples with low silhouette values (`removeSil`). The default cutoff for the minimum silhouette width is 0. This cutoff can be adjusted via the parameter `silCutoff`.

We can pass these arguments to `clusterD`, again as a list:

```{r visualizeArgsClusterD, echo=FALSE}
drawClusterAll(highlight = c("pam", "subsample.FALSE", "sequential.FALSE"),Layer2Options=list("clusterDArgs"=c("k","findBestK","removeSil","kRange")),plotOptions=TRUE)
```

```{r clusterD}
Cluster<-clusterSingle(expression, subsample=FALSE, sequential=FALSE, clusterFunction="pam", clusterDArgs=list(findBestK=TRUE, removeSil=TRUE, kRange=2:10))

table(true=expressions$cluster, PAM=Cluster$clustering)
```

Note that, even in this toy example, it is not easy to choose the correct number of clusters, and a simple procedure based on comparing average silhouette width may not always lead to good results.

**Note:** at the current time, if we are not going to do the subsample task (`subsample=FALSE`), then `clusterFunction` *must* be `pam` because the other clustering options for `clusterFunction` assume a 0-1 distance matrix based on subsampling. If there is no subsampling, `clusterSingle` makes the distance matrix by default with `dist(x)` and therefore must be used with `pam`, as this is the only implemented method for distance matrices.

## Subsampling

We can add the additional task to our clustering workflow to subsample the samples and cluster the resampled data. This is accomplished by setting `subsample=TRUE`. In this case, `clusterSingle` will first subsample the rows of the data matrix, cluster the subsamples for a specified clustering algorithm, and return a clustering co-occurance matrix. 

Then, `clusterSingle` will cluster this co-occurance matrix using another specified algorithm, which can be the same as the algorithm used in clustering the subsampled data, but doesn't have to be. Indeed, several of the cluster algorithm options (`tight` and `hierarchical`) are designed to cluster 0-1 co-occurance matrices from subsampling. 

In this demonstration, let us cluster the subsamples using `kmeans` with `k=5`, then cluster the co-occurance matrix using the tight clustering algorithm. Now we have options we want to set for the subsampling portion of the task. 

```{r visualizeSubsample, echo=FALSE}
drawClusterAll(highlight = c("tight", "subsample.TRUE", "sequential.FALSE"),Layer2Options=list("subsampleArgs"=c("k","clusterFunction")),plotOptions=TRUE)
```
```{r clusterSingleSubsample}
set.seed(212421)
Cluster<-clusterSingle(expression, subsample=TRUE, sequential=FALSE, clusterFunction="tight", subsampleArgs=list("k"=5, clusterFunction="kmeans"))

table(true=expressions$cluster, subsample=Cluster$clustering)
```

As expected, the subsampling procedure will return more clusters than a simple PAM. In addition, some observations may be not clustered (value of `-1`), because the proportion of times that they get assign to the same cluster core across subsamplings does not pass the specified threshold. Note that the choice of `k` is now not directly linked to the output labels: although we selected `k=5` for the subsampling step, the final result contains 8 clusters.

_DR: we should probably show a heatmap of the subsampling proportions to make this point clearer_

## Sequential 

The last additional task is to sequentially remove the best cluster, and then recluster the remaining samples. This is performed using  `sequential=TRUE`.

Again, we need to pass arguments to control how this is done, as a list to the argument `seqArgs`. A necessary argument `k0` must be passed to `seqArgs`. What this argument means depends on what other choices are being made, but if we also set `subsample=TRUE` (recommended), then `k0` is the value of k passed to the subsampling clustering method at the first iteration of sequential algorithm.

This time we will use the hierarchical method for clustering the 0-1 co-occurance matrix. 

```{r visualizeSeq, echo=FALSE}
drawClusterAll(highlight = c("hierarchical", "subsample.TRUE", "sequential.TRUE"),Layer2Options=list("subsampleArgs"=c("clusterFunction"),seqArgs=c("k0","verbose")),plotOptions=TRUE)
```

```{r clusterSingleSeq}
Cluster<-clusterSingle(expression, subsample=TRUE, sequential=TRUE, clusterFunction="hierarchical", seqArgs=list(k0=5, verbose = FALSE), subsampleArgs = list(clusterFunction="kmeans"))
```

Running `clusterSingle` with `sequential=TRUE` will return a list of the three elements `clustering`, `clusterInfo`, and `whyStop`. `clusterInfo` is a matrix of information regarding the algorithm behavior for each cluster (the starting and stopping k set for subsampling clustering for each cluster, and the number of iterations for each cluster). `whyStop` is a character string explaining what triggered the algorithm to stop.

```{r seqOut}
table(Cluster$clustering)
Cluster$clusteringInfo
Cluster$whyStop

table(true=expressions$cluster, clusterExperiment=Cluster$clustering)
```

This procedure leads to many more clusters, many of which are very small size.

_DR: don't we have a min size parameter to remove clusters with too few observations? Also, here would be a good place to talk about merging. Perhaps merging before finding markers?_

